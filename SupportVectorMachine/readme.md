# ðŸ§  Support Vector Machine (SVM) Classifier â€“ Visualized and Explained

This notebook demonstrates the power of Support Vector Machines (SVM) for classification tasks, with a focus on **visualizing the decision boundary**, **understanding support vectors**, and exploring the effects of **different kernels** and **regularization (`C`) values**.

## ðŸ“Œ Key Features

- ðŸ” **Linear and Non-linear Classification**
  - Visualization of hyperplanes in 2D feature space
  - Use of `linear` and `rbf` kernels

- ðŸ“‰ **Class Balancing**
  - Undersampling of the majority class (`Class = 0`) to improve visualization and fairness

- âš–ï¸ **Feature Normalization**
  - Use of `MinMaxScaler` to bring all features to the same scale for clearer decision boundaries

- ðŸ§ª **Kernel & C Parameter Experimentation**
  - Grid-style exploration of different `C` values and kernel types (`linear`, `rbf`, `poly`, `sigmoid`)
  - Evaluation with accuracy, precision, recall, and F1 score

- ðŸ“Š **Visualization Utility**
  - A reusable function to plot:
    - Decision boundary (hyperplane)
    - Margins
    - Support vectors
    - Class separation

## ðŸ“ˆ Evaluation Metrics

Each model is assessed using:

- **Accuracy** â€“ Overall correctness
- **Recall** â€“ Ability to find all positive cases
- **Precision** â€“ Reliability of positive predictions
- **F1 Score** â€“ Balance between precision and recall

## ðŸ› ï¸ Technologies Used

- Python 3
- Scikit-learn
- NumPy
- Matplotlib / Seaborn
- Jupyter Notebook

## ðŸ“¸ Sample Output

![SVM Decision Boundary Example](https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png)

> *The margin is maximized, and support vectors define the boundary â€” curved when using non-linear kernels like RBF.*

## ðŸ’¡ Learning Outcome

This notebook serves as both a visual and practical introduction to how SVM works. Itâ€™s ideal for students, machine learning practitioners, and professionals seeking to understand SVM intuitively.
